1.  The data source I chose is the six hour average cosmic ray particle flux data collected by the Cosmic Ray System (CRS) on   
    board Voyager-1. This data has been collected by the spacecraft since its launch in 1977 and it is expected that it will keep on collecting this data till 2025, when its RTG won't generate sufficient power for the CRS to operate. 
    The study of cosmic rays, which are highly energetic particles definitely not of solar origin, is the key to understanding their    origin, the cause of their high energies and derivative topics like the origin of the Universe, the formation of stars and the    production of elements (nucleosynthesis) in the core of stars. 
    Most of the instruments on board Voyager-1 were shut down in 1990s except for the CRS, Low Energy Charged Particle Instrument    (LECP), and Plasma Wave Subsystem (PWS). The data from these three remaining systems was crucial in the confirmation of the fact    that Voyager-1 has crossed the termination shock of the Sun and entered interstellar space in 2012. The six hour average cosmic ray    particle flux data was very helpful in this process. 
    Although I am studying Electrical and Electronics Engineering (Bachelor's Degree), I am very interested in physics and therefore    want to analyze this data with the limited understanding of particle physics that I have. 
    The link for this data, which is my data source for this project is:    https://spdf.gsfc.nasa.gov/pub/data/voyager/voyager1/particle/crs/six_hour/
    One can even go to  https://omniweb.gsfc.nasa.gov/ and get access to data collected by other spacecrafts launched by NASA. 

2.  The data source I have chosen for this project is the six hour average cosmic ray particle flux data, collected by Cosmic 
    Ray System (CRS) on board Voyager-1, from 1977 (the year when the spacecraft was launched) to 2019. This data is available at the following URL: https://spdf.gsfc.nasa.gov/pub/data/voyager/voyager1/particle/crs/six_hour/. One will see that the data for each year is present in a seperate file with .asc extension. For example, the data for the year 1999 is present in the file named vy1crs_6hour_1999.asc having the URL https://spdf.gsfc.nasa.gov/pub/data/voyager/voyager1/particle/crs/six_hour/vy1crs_6hour_1999.asc. 
    I have broken the process of retrieval and cleaning of this data into two seperate sub-processes: Understanding the format in which this data is stored, or the meaning of each record in the data for a particular year, and finally dumping of this data into a database in my computer. The final analysis of this data, which is done using plots of cosmic ray particle flux vs time, was done using this database, and scientific computing libraries of Python, namely numpy and matplotlib. 
    To interpret this data correctly, Dr. Nand Lal, of Goddard Space Flight Center, has written a manual (in plain ASCII text file format) which is available at the following URL: https://spdf.gsfc.nasa.gov/pub/data/voyager/voyager1/particle/crs/six_hour/vy1crs_6h_fmt.txt
    I will not be discussing the interpretation of the data here, or the details about the CRS. However, I found it quite interesting that in this manual an example program in Fortran-77 to retrieve this data has been provided. This program is written in Fortran-77, which was in use when Voyager-1 and 2 were launched. This data was stored in IBM mainframes in magnetic tape records. Another thing worth mentioning here is that since the probe is 20 light hours away from us, it takes about 20 hours for it to transmit this data back to Earth. Naturally, given the age of the probe and the distance between it and us, there are bound to be errors in the measurements taken by the CRS. These errors are given alongside fluxes of cosmic ray particles of different energies. For further detail, please refer to the aforementioned document. 
    Now I will talk about the second sub-process which is the dumping of this data into a database in my computer. In Dr. Chuck's classes, we were often told how if our Internet service provider or the host detects that we have been extracting data from their server beyond a certain time limit, they will close off our connection with the server. Well, this happened with me two times. Initially I thought that it would be better, if I just put the data directly into a database. However, the process of committing such a huge amount of data to the database takes a lot of time. On my first attempt, after 4 hours of extracting data from 1977 to 1988 I crossed my data usage limit. On my second attempt, after almost 6 hours, I was able to extract data from 1977 to 2013 but the host server blocked my connection due to a mismatch in the time stamps of the data sent by the host and the data received by my computer. Clearly, time taken to recieve a data and commit it into the computer's memory was a critical factor. Then I remembered how although it is difficult to extract a specific data record from a text file, it is fairly easy to write data to one. The opposite is true for a relational database: storing data into it from some source takes some time, but extracting it is very fast. So my plan was this: dump (literally) the data from the host server into a text file in my computer. Then from this text file, insert the data into an SQL database. The first part will be done in no time and I won't be kicked out of the link either by the ISP or by the host server. The second part will take some time to get completed but it won't be requiring any Internet connection. The first part (dumping of data into a text file) took me about 2-3 hours (I did not note down the time it took for this process to get completed). The second part, which was the insertion of data from a text file into an SQL database, took about 6 hours, which was expected. This gave me a database in which there are three tables. The first table is Bin and it contains the energy range of particles whose flux and statistical error is stored in a given Bin number. The second table is Flux and it contains N Bins, each containing the flux of particles in a given energy range. The third table is Error and it contains N Bins, each bin containing the statistical error in the flux measurement of the particle in a given bin. 
    I will give a small example here. Here is a small piece of record of average six hour flux data collected on 2000-01-05T18:00:00.
    31
    1 3.000 - 4.600 MeV/n Hydrogen flux
    2 4.600 - 6.200 MeV/n Hydrogen flux
    3 6.200 - 7.700 MeV/n Hydrogen flux
    ...
    ...
    ...
    2000-01-05T18:00:00 1.215e+00 1.215e+00 2.540e+00 1.270e+00 2.217e+00 1.252e+00... 
    ...
    ...
    ...
    31 means that there are 31 bins in total. Next line tells us that bin 1 contains flux and error related to Hydrogen ions (a strange way of saying protons) having energy in the range 3.000 to 4.600 MeV per nucleon. Next line tells us that bin 1 contains flux and error related to protons having energy in the range 4.600 to 6.200 MeV per nucleon.
    The first data bit gives the time when this data was recorded. The next two data bits constitute Bin no. 1. We read it as follows: Flux of protons having energies in the range 3.000 to 4.600 MeV/n was 1.215e+00 particles per centimetre square per second per steradian per MeV/n (particles/cm**2.s.sr.Mev/n). Other bins can be interpreted in a similar manner. 
    The main work of dumping the data from the host server into a text file and using the data in the text file to construct a database was done by two programs, voyager.py and vdb3_0.py (where 3_0 means that this is the third version of the program meant to do this job). voyager.py contains functions which were used to generte SQL commands, URLs, etc. These functions were imported to vdb3_0.py using "from voyager import *" statement.
    I should mention over here a glitch which I encountered. After the database was populated, I observed that the data from 2019-11-23T18:00:00 to 2019-11-25T18:00:00 was not entered into the database. So to enter it into the database, I wrote a small program, vdb_crude.py, which wrote the missing data into the database. 
    In the end I got my biggest database ever, in which the Flux and Error table both have 45855 rows and 32 columns. 
    
3.  With the average six hour cosmic ray particle flux data collected by the CRS on board Voyager-1 from 1977 to 2019 now in an
    SQL database in my computer, the only part left to do in this project was its visualization. The visualization involved plotting of the particle flux corresponding to a specific energy range and particle type (or in short, a specific bin number) with respect to time. So on the horizontal axis will be time with graduations shown for years, and on the vertical axis will be the average flux of particles/cm**2.s.sr.Mev/n. To plot the data in this way, the first step was to calculate the total number of bins. This was done easily by printing the entire Bin table. After this a prompt asks the user to enter the time range in YYYY-MM-DD HH-mm-ss format for which the plot is to be created. Then a prompt asks the user to enter all the bins he wishes to plot. Now this was the part where I tried to be practical as well as be creative. A physicist might want to compare the plots of two different particle fluxes or just plot the total flux of over a given  range of energy. In other words, a plot of flux of particles in the required energy range might contain fluxes in several bins. For example, suppose we require to plot the flux of particles in the energy range 3.000 MeV/n to 12.800 MeV/n. If one looks at the Bin table, this requirement clearly means that the fluxes in bin 1 to 4 having the same timestamp should be added up and be marked over that timestamp on the graph. Therefore the user will take one of the following two paths when using this program: he will either plot particle fluxes in a given energy range or plot particle fluxes in two different energy ranges and compare them, for which he will have to construct two seperate plots. To solve this problem, I developed a notation which the user has to use to enter the bin numbers that are required to be plotted. The notation is pretty simple: it consists of only two non-numerical symbols, a plus sign (+) and a colon sign(:). Colon seperates the two energy ranges and the plus sign on either side of the colon mean that the fluxes in those bins are to be added. Let us take an example. Suppose we wish to plot fluxes in bin 1, 2, 3, and 4. The user then simply has to enter the following string:
    1 + 2 + 3 + 4
    Thanks to Regex, one does not worry about the spaces between the plus sign and the bin numbers. Therefore the strings 1+2+3+ 4  and 1 + 2 + 3 + 4 are interpreted similarly. Suppose the user wishes to compare the flux data corresponding to energy range of bin 1, 2, 3, 4 with those of bin 9, 10, 11, and 12. Then the string the user has to enter is:
    1 + 2 + 3 + 4 : 9 + 10 + 11 + 12 
    I added another feature in this user interface. Suppose the user wishes to see a plot of flux of all the bins having the particle type as Hydrogen or Helium. The the user simply has to type 'Hydrogen' or 'Helium'. Note that the case type does matter. Well, others can modify voyager.py to make this command interpretation independent of case type, and also one can easily modify the program to interpret the symbol for Hydrogen and Helium, i.e., H and He, respectively. I am just too lazy to deal with this now. 
    Once this is done, four one dimensional numerical arrays using numpy.array([]) command are created. Normal arrays or lists are not used as although they can be used to plot the fluxes with respect to time, they are slower as compared to Numpy arrays when it comes to mathematical computations. Numpy array object are specifically designed for mathematical compuation purposes only. Two arrays store the time range on which the two fluxes to be compared are to be plotted. The other two arrays store the flux corresponding to the timestamp in the time arrays. Then a simple if-else conditional statement is used to check whether a comparison is being made or not and accordingly the plots are produced. 
    Now let us analyze the plot of Hydrogen flux from 1999-01-01 00:00:00 to 2019-01-01 00:00:00. We observe how the flux suddenly rose in 2004. This meant the Voyager-1 had entered the termination shock of the Sun. Here the solar wind blows at subsonic speeds and magnetic fields are different than in the interplanetary region. It was due to this that the flux of protons, which are positively charged and hence are affected by magnetic fields, has increased considerably (almost by a factor of 2000 initially). Then observe how around the end of 2012 this flux dropped to very low levels. This meant that Voyager-1 has crossed the heliosphere of the Sun, which is the bubble created by the solar winds. It was the analysis of this drop in Hydrogen flux which made scientists realize that Voyager-1 has entered interstellar space. 
    Now one can go to the following URL: https://omniweb.gsfc.nasa.gov/ftpbrowser/vy1_crs_6h_flux.html and get these plots. One will now ask, as to what is the uttility of my program? It has already been made before! Well, first of all, it was an exercise in using all that I have learnt in this online course. Secondly, with this data now in my laptop, I can access it and analyze it whenever I like to do so. In the end, to the person who is still not satisfied with my answers (on why I did this project), I remember an anecdote about Michael Faraday. When he told his friends about his discovery of electromagnetic induction, one of them asked him about the use of the sparks which were the result of the voltage induced in the coil due to a magnet moving near it. Faraday replied, "What is the use of an infant?". To the person who does not quite understands the utility of my project, I ask "What is the utility of a newborn baby? Aren't there enough already in this world!" 